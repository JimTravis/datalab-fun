{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "#!tar xvf simple-examples.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "  with gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data]\n",
    "\n",
    "\n",
    "def ptb_raw_data():\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join('./simple-examples/data/', \"ptb.train.txt\")\n",
    "  valid_path = os.path.join('./simple-examples/data/', \"ptb.valid.txt\")\n",
    "  test_path = os.path.join('./simple-examples/data', \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_scale = 0.1\n",
    "learning_rate = 1.0\n",
    "max_grad_norm = 5\n",
    "num_layers = 5\n",
    "num_steps = 20\n",
    "hidden_size = 200\n",
    "max_epoch = 4\n",
    "max_max_epoch = 13\n",
    "keep_prob = 0.5\n",
    "lr_decay = 0.5\n",
    "batch_size = 20\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding:0\n",
      "RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix:0\n",
      "RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias:0\n",
      "RNN/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix:0\n",
      "RNN/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias:0\n",
      "RNN/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix:0\n",
      "RNN/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias:0\n",
      "RNN/MultiRNNCell/Cell3/BasicLSTMCell/Linear/Matrix:0\n",
      "RNN/MultiRNNCell/Cell3/BasicLSTMCell/Linear/Bias:0\n",
      "RNN/MultiRNNCell/Cell4/BasicLSTMCell/Linear/Matrix:0\n",
      "RNN/MultiRNNCell/Cell4/BasicLSTMCell/Linear/Bias:0\n",
      "softmax_w:0\n",
      "softmax_b:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "\n",
    "  '''Placeholders for'''\n",
    "  input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "  targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "  lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n",
    "  lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "  cell = rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "\n",
    "  initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "  initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "\n",
    "  embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size], initializer=initializer)      \n",
    "  \n",
    "  inputs_tmp1 = tf.nn.embedding_lookup(embedding, input_data) # [20][20][200]\n",
    "  inputs_tmp2 = tf.nn.dropout(inputs_tmp1, keep_prob) # [20][20][200]\n",
    "  inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs_tmp2)] # (list of 20)[20][200]\n",
    "    \n",
    "  outputs, states = rnn.rnn(cell, inputs, initial_state=initial_state)\n",
    "  \n",
    "  output = tf.reshape(tf.concat(1, outputs), [-1, hidden_size])\n",
    "  logits = tf.nn.xw_plus_b(output,\n",
    "                           tf.get_variable(\"softmax_w\", [hidden_size, vocab_size]),\n",
    "                           tf.get_variable(\"softmax_b\", [vocab_size]))\n",
    "  loss = seq2seq.sequence_loss_by_example([logits],\n",
    "                                          [tf.reshape(targets, [-1])],\n",
    "                                          [tf.ones([batch_size * num_steps])],\n",
    "                                          vocab_size)\n",
    "  cost = tf.reduce_sum(loss) / batch_size\n",
    "  final_state = states[-1]\n",
    "  \n",
    "  lr = tf.Variable(0.0, trainable=False) # trainable defaults to True..!\n",
    "  tvars = tf.trainable_variables()\n",
    "  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "  train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  \n",
    "  raw_data = ptb_raw_data()\n",
    "  train_data, valid_data, test_data, _ = raw_data\n",
    "  \n",
    "  for i in range(max_max_epoch):\n",
    "    \n",
    "    lr_decay = lr_decay ** max(i - max_epoch, 0.0)\n",
    "    session.run(tf.assign(lr, learning_rate * lr_decay))\n",
    "    \n",
    "    epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    loop_state = initial_state.eval()\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_iterator(train_data, batch_size, num_steps)):\n",
    "  \n",
    "      ret1, ret2, ret3, ret4, ret5, ret6, ret7, loop_cost, loop_state, _ = session.run([inputs_tmp1, \n",
    "                                embedding,\n",
    "                                inputs_tmp2,\n",
    "                                inputs[0],\n",
    "                                outputs[0],\n",
    "                                output,\n",
    "                                logits,\n",
    "                                cost, final_state, train_op],\n",
    "                 {input_data: x,\n",
    "                  targets: y,\n",
    "                  initial_state: loop_state})\n",
    "    \n",
    "      for v in tvars:\n",
    "        print(v.name)\n",
    "      \n",
    "      break\n",
    "\n",
    "      '''loop_cost, loop_state, _ = session.run([cost, final_state, train_op],\n",
    "                                   {input_data: x,\n",
    "                                    targets: y,\n",
    "                                    initial_state: loop_state})\n",
    "\n",
    "      costs += loop_cost\n",
    "      iters += num_steps\n",
    "\n",
    "      if step % (epoch_size // 10) == 10:\n",
    "        print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "              (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "               iters * batch_size / (time.time() - start_time)))\n",
    "\n",
    "    perplexity = np.exp(costs / iters)'''\n",
    "      \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 200)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 200)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 200)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 200)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 200)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.50319203e-04,   1.06235539e-05,  -6.09649906e-05, ...,\n",
       "          9.13511685e-05,   0.00000000e+00,   1.14943025e-04],\n",
       "       [ -0.00000000e+00,  -0.00000000e+00,  -1.59318108e-04, ...,\n",
       "          0.00000000e+00,  -2.63182068e-04,   2.08657380e-04],\n",
       "       [  0.00000000e+00,  -9.24876585e-05,  -0.00000000e+00, ...,\n",
       "          4.72031476e-04,   4.13459202e-05,  -0.00000000e+00],\n",
       "       ..., \n",
       "       [ -8.33519385e-04,  -0.00000000e+00,  -0.00000000e+00, ...,\n",
       "          1.16604591e-04,   9.02963395e-04,  -1.63923600e-04],\n",
       "       [ -8.11368867e-04,   3.21977532e-05,  -0.00000000e+00, ...,\n",
       "         -0.00000000e+00,   0.00000000e+00,   5.17878332e-04],\n",
       "       [ -0.00000000e+00,   0.00000000e+00,  -3.43326945e-04, ...,\n",
       "         -1.44983263e-04,   5.53562888e-04,   0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 10000)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42348099, -0.26516798,  1.55406642, ...,  0.04979973,\n",
       "        1.68157017,  0.15827167], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret7[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to eval(session=sess).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-fa401f16bdfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtvars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, session)\u001b[0m\n\u001b[0;32m    231\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m`\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \"\"\"\n\u001b[1;32m--> 233\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minitialized_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \"\"\"\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   2717\u001b[0m                        \"eval(session=sess)\")\n\u001b[0;32m   2718\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2719\u001b[1;33m       raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n\u001b[0m\u001b[0;32m   2720\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2721\u001b[0m                        \u001b[1;34m\"graph. Pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to eval(session=sess)."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  for v in tvars:\n",
    "    print(v.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start to think about applying this to financial time series...\n",
    "What should be the inputs..?\n",
    "And what should be the outputs..?\n",
    "\n",
    "Inputs:\n",
    "* A tensor of each index from day N.  E.g. [1.23, 2.34, 3.45, 4.56, 5.67, 6.78, 7.89, 8.90]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
