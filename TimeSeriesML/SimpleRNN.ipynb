{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "#!tar xvf simple-examples.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for parsing PTB text files.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "def _read_words(filename):\n",
    "  with gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data]\n",
    "\n",
    "\n",
    "def ptb_raw_data():\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join('./simple-examples/data/', \"ptb.train.txt\")\n",
    "  valid_path = os.path.join('./simple-examples/data/', \"ptb.valid.txt\")\n",
    "  test_path = os.path.join('./simple-examples/data', \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This generates batch_size pointers into the raw PTB data, and allows\n",
    "  minibatch iteration along these pointers.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "  Yields:\n",
    "    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].\n",
    "    The second element of the tuple is the same data time-shifted to the\n",
    "    right by one.\n",
    "  Raises:\n",
    "    ValueError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    "\n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    "\n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "\n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "\n",
    "class PTBModel(object):\n",
    "\n",
    "  def __init__(self, is_training, config):\n",
    "    \n",
    "    '''batch_size and num_steps are fundamental concepts in our model.\n",
    "    batch_size controls how much data you are considering at a time - it helps control the computational size of training.\n",
    "    num_steps controls the degree of unrolling - it truncates back propagation.'''\n",
    "    self.batch_size = config.batch_size\n",
    "    self.num_steps = config.num_steps\n",
    "\n",
    "    '''Placeholders for input and output data.'''\n",
    "    self.input_data = tf.placeholder(tf.int32, [self.batch_size, self.num_steps])\n",
    "    self.targets = tf.placeholder(tf.int32, [self.batch_size, self.num_steps])\n",
    "\n",
    "    '''START NET STRUCTURE'''\n",
    "    '''Set up an RNN using LSTM cells of size config.hidden_size with dropout probability of 1/config.keep_prob\n",
    "    and config.num_layers layers.'''\n",
    "    '''Basic LSTM cell with config.hidden_size units.  forget_bias is set to 0.0 rather than 1.0 (default).\n",
    "    What does forget_bias mean?'''\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n",
    "    \n",
    "    '''If this is training (and the probability indicates dropout) then add dropout to the cell'''\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      lstm_cell = rnn_cell.DropoutWrapper(\n",
    "          lstm_cell, output_keep_prob=config.keep_prob)\n",
    "      \n",
    "    '''If config indicates multiple, stacked units set this up.'''\n",
    "    cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)\n",
    "    '''END NET STRUCTURE'''\n",
    "\n",
    "    '''Zero the initial state.'''\n",
    "    self.initial_state = cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "    '''Device is the processing unit for the session/context manager.  Why do we need it..?'''\n",
    "    '''The embedding is the key structure.  For each word we learn words that are semantically close to it.\n",
    "    They will then be the most probable following words.'''\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      '''Creates (or looks up) the embedding for the word/ID representation.  If it creates it will be initialized randomly.'''\n",
    "      embedding = tf.get_variable(\"embedding\", [config.vocab_size, config.hidden_size])      \n",
    "      inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "    '''If training and probability indicates dropout then perform dropout on inputs.'''\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "    '''tf.split(1, self.num_steps, inputs) - splits inputs tensor into num_steps tensors.'''\n",
    "    '''tf.squeeze(input_, [1]) - removes dimensions of size 1 from input_.  What does this DO..?'''\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, self.num_steps, inputs)]\n",
    "    \n",
    "    outputs, states = rnn.rnn(cell, inputs, initial_state=self.initial_state)\n",
    "\n",
    "    output = tf.reshape(tf.concat(1, outputs), [-1, config.hidden_size])\n",
    "    logits = tf.nn.xw_plus_b(output,\n",
    "                             tf.get_variable(\"softmax_w\", [config.hidden_size, config.vocab_size]),\n",
    "                             tf.get_variable(\"softmax_b\", [config.vocab_size]))\n",
    "    loss = seq2seq.sequence_loss_by_example([logits],\n",
    "                                            [tf.reshape(self.targets, [-1])],\n",
    "                                            [tf.ones([self.batch_size * self.num_steps])],\n",
    "                                            config.vocab_size)\n",
    "    self.cost = cost = tf.reduce_sum(loss) / self.batch_size\n",
    "    self.final_state = states[-1]\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    self.lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                      config.max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "    self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 2 #20\n",
    "  hidden_size = 200\n",
    "  max_epoch = 4\n",
    "  max_max_epoch = 13\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "def run_epoch(session, m, data, eval_op, verbose=False):\n",
    "  \n",
    "  epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "  start_time = time.time()\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  state = m.initial_state.eval()\n",
    "  \n",
    "  for step, (x, y) in enumerate(ptb_iterator(data, m.batch_size,\n",
    "                                                    m.num_steps)):\n",
    "    \n",
    "    cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n",
    "                                 {m.input_data: x,\n",
    "                                  m.targets: y,\n",
    "                                  m.initial_state: state})\n",
    "    \n",
    "    costs += cost\n",
    "    iters += m.num_steps\n",
    "\n",
    "    if verbose and step % (epoch_size // 10) == 10:\n",
    "      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "             iters * m.batch_size / (time.time() - start_time)))\n",
    "\n",
    "  return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = ptb_raw_data()\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "config = Config()\n",
    "\n",
    "eval_config = Config()\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "  initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "  \n",
    "  with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "    m = PTBModel(is_training=True, config=config)\n",
    "    \n",
    "  with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "    mvalid = PTBModel(is_training=False, config=config)\n",
    "    mtest = PTBModel(is_training=False, config=eval_config)\n",
    "    \n",
    "  tf.initialize_all_variables().run()\n",
    "  \n",
    "  #for i in range(config.max_max_epoch):\n",
    "  for i in range(1):\n",
    "    lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n",
    "    m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "    print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "    train_perplexity = run_epoch(session, m, train_data, m.train_op, verbose=True)\n",
    "    print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "    valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "    print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "    \n",
    "  test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
    "  print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_scale = 0.1\n",
    "learning_rate = 1.0\n",
    "max_grad_norm = 5\n",
    "num_layers = 5\n",
    "num_steps = 20\n",
    "hidden_size = 200\n",
    "max_epoch = 4\n",
    "max_max_epoch = 13\n",
    "keep_prob = 0.5\n",
    "lr_decay = 0.5\n",
    "batch_size = 20\n",
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "\n",
    "  input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "  targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "\n",
    "  lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n",
    "  lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "  cell = rnn_cell.MultiRNNCell([lstm_cell] * num_layers)\n",
    "\n",
    "  initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "  initializer = tf.random_uniform_initializer(-init_scale, init_scale)\n",
    "\n",
    "  embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size], initializer=initializer)      \n",
    "  \n",
    "  inputs_tmp1 = tf.nn.embedding_lookup(embedding, input_data)\n",
    "  inputs_tmp2 = tf.nn.dropout(inputs_tmp1, keep_prob)\n",
    "  inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs_tmp2)]\n",
    "    \n",
    "  outputs, states = rnn.rnn(cell, inputs, initial_state=initial_state)\n",
    "  \n",
    "  output = tf.reshape(tf.concat(1, outputs), [-1, hidden_size])\n",
    "  logits = tf.nn.xw_plus_b(output,\n",
    "                           tf.get_variable(\"softmax_w\", [hidden_size, vocab_size]),\n",
    "                           tf.get_variable(\"softmax_b\", [vocab_size]))\n",
    "  loss = seq2seq.sequence_loss_by_example([logits],\n",
    "                                          [tf.reshape(targets, [-1])],\n",
    "                                          [tf.ones([batch_size * num_steps])],\n",
    "                                          vocab_size)\n",
    "  cost = tf.reduce_sum(loss) / batch_size\n",
    "  final_state = states[-1]\n",
    "  \n",
    "  lr = tf.Variable(0.0, trainable=False)\n",
    "  tvars = tf.trainable_variables()\n",
    "  grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "  train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "  \n",
    "  session.run(tf.initialize_all_variables())\n",
    "  \n",
    "  raw_data = ptb_raw_data()\n",
    "  train_data, valid_data, test_data, _ = raw_data\n",
    "  \n",
    "  for i in range(max_max_epoch):\n",
    "    \n",
    "    lr_decay = lr_decay ** max(i - max_epoch, 0.0)\n",
    "    session.run(tf.assign(lr, learning_rate * lr_decay))\n",
    "    \n",
    "    epoch_size = ((len(train_data) // batch_size) - 1) // num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    loop_state = initial_state.eval()\n",
    "\n",
    "    for step, (x, y) in enumerate(ptb_iterator(train_data, batch_size, num_steps)):\n",
    "  \n",
    "      ret1, ret2, ret3, ret4 = session.run([inputs_tmp1, \n",
    "                                embedding,\n",
    "                                inputs_tmp2,\n",
    "                                inputs[0]],\n",
    "                 {input_data: x,\n",
    "                  targets: y,\n",
    "                  initial_state: loop_state})\n",
    "      \n",
    "      break\n",
    "\n",
    "      '''loop_cost, loop_state, _ = session.run([cost, final_state, train_op],\n",
    "                                   {input_data: x,\n",
    "                                    targets: y,\n",
    "                                    initial_state: loop_state})\n",
    "\n",
    "      costs += loop_cost\n",
    "      iters += num_steps\n",
    "\n",
    "      if step % (epoch_size // 10) == 10:\n",
    "        print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "              (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "               iters * batch_size / (time.time() - start_time)))\n",
    "\n",
    "    perplexity = np.exp(costs / iters)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20, 200)\n",
      "(10000, 200)\n",
      "(20, 20, 200)\n",
      "(20, 200)\n",
      "(20, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "print(ret1.shape)\n",
    "print\n",
    "print(ret2.shape)\n",
    "print\n",
    "print(ret3.shape)\n",
    "print\n",
    "print(ret4.shape)\n",
    "print\n",
    "print(ret5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.06976767, -0.        , -0.        , -0.18383498,\n",
       "         0.04760842,  0.        , -0.19403587, -0.        , -0.06934309,\n",
       "        -0.        ,  0.        ,  0.00987712, -0.        , -0.        ,\n",
       "         0.        , -0.        ,  0.        , -0.18004762,  0.04186821,\n",
       "         0.        ,  0.17032333, -0.10798144,  0.13395394,  0.0044004 ,\n",
       "         0.        , -0.        , -0.        , -0.        , -0.00241742,\n",
       "        -0.06923652,  0.        , -0.04678969,  0.        , -0.0671256 ,\n",
       "        -0.        , -0.        ,  0.15680198, -0.        , -0.07658863,\n",
       "         0.        ,  0.03074665,  0.00498104,  0.        ,  0.05465998,\n",
       "        -0.19333978, -0.13523045,  0.04179063, -0.14354853,  0.02281737,\n",
       "         0.        ,  0.        , -0.15542312, -0.04228082,  0.14731266,\n",
       "         0.        , -0.        , -0.11571407, -0.        , -0.13928676,\n",
       "        -0.08894677, -0.0172914 , -0.        ,  0.09594561,  0.09298982,\n",
       "        -0.09191232, -0.        , -0.        ,  0.        , -0.        ,\n",
       "        -0.        , -0.        ,  0.06047107,  0.12795378, -0.12439647,\n",
       "         0.05277567,  0.        , -0.17392449, -0.        , -0.09721231,\n",
       "        -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "         0.01934357,  0.0037148 ,  0.03329673, -0.03734283, -0.07190594,\n",
       "        -0.08483453, -0.06224346,  0.        ,  0.        , -0.14582086,\n",
       "         0.        ,  0.150622  , -0.1644372 ,  0.        , -0.16660395,\n",
       "         0.16297178, -0.07108431,  0.        ,  0.        , -0.0658925 ,\n",
       "        -0.18245578, -0.        , -0.        ,  0.        ,  0.08056222,\n",
       "         0.0656362 ,  0.18067522,  0.00173946,  0.        ,  0.02065158,\n",
       "         0.02433662, -0.        ,  0.1767727 ,  0.        ,  0.01161762,\n",
       "        -0.        , -0.        , -0.18004146, -0.        ,  0.0147099 ,\n",
       "         0.        ,  0.        ,  0.06646849, -0.16067734,  0.        ,\n",
       "         0.        ,  0.10267164,  0.11150776,  0.15527655,  0.        ,\n",
       "         0.        ,  0.18290897,  0.        ,  0.11125077,  0.14937095,\n",
       "        -0.        , -0.00420108,  0.16608955,  0.14837117,  0.        ,\n",
       "         0.09318213, -0.15986633, -0.        ,  0.04242773,  0.        ,\n",
       "        -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "        -0.09592042,  0.        , -0.        , -0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.02152976, -0.        , -0.        ,\n",
       "        -0.10259014, -0.16574502, -0.        ,  0.        , -0.        ,\n",
       "        -0.        ,  0.        , -0.        ,  0.        , -0.17510557,\n",
       "         0.01584105, -0.05838689, -0.        , -0.        , -0.17603575,\n",
       "         0.        ,  0.        , -0.        , -0.1465838 ,  0.        ,\n",
       "        -0.        , -0.18253402,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -0.        , -0.        ,  0.0575297 ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.06976767, -0.        , -0.        , -0.18383498,\n",
       "        0.04760842,  0.        , -0.19403587, -0.        , -0.06934309,\n",
       "       -0.        ,  0.        ,  0.00987712, -0.        , -0.        ,\n",
       "        0.        , -0.        ,  0.        , -0.18004762,  0.04186821,\n",
       "        0.        ,  0.17032333, -0.10798144,  0.13395394,  0.0044004 ,\n",
       "        0.        , -0.        , -0.        , -0.        , -0.00241742,\n",
       "       -0.06923652,  0.        , -0.04678969,  0.        , -0.0671256 ,\n",
       "       -0.        , -0.        ,  0.15680198, -0.        , -0.07658863,\n",
       "        0.        ,  0.03074665,  0.00498104,  0.        ,  0.05465998,\n",
       "       -0.19333978, -0.13523045,  0.04179063, -0.14354853,  0.02281737,\n",
       "        0.        ,  0.        , -0.15542312, -0.04228082,  0.14731266,\n",
       "        0.        , -0.        , -0.11571407, -0.        , -0.13928676,\n",
       "       -0.08894677, -0.0172914 , -0.        ,  0.09594561,  0.09298982,\n",
       "       -0.09191232, -0.        , -0.        ,  0.        , -0.        ,\n",
       "       -0.        , -0.        ,  0.06047107,  0.12795378, -0.12439647,\n",
       "        0.05277567,  0.        , -0.17392449, -0.        , -0.09721231,\n",
       "       -0.        ,  0.        , -0.        , -0.        , -0.        ,\n",
       "        0.01934357,  0.0037148 ,  0.03329673, -0.03734283, -0.07190594,\n",
       "       -0.08483453, -0.06224346,  0.        ,  0.        , -0.14582086,\n",
       "        0.        ,  0.150622  , -0.1644372 ,  0.        , -0.16660395,\n",
       "        0.16297178, -0.07108431,  0.        ,  0.        , -0.0658925 ,\n",
       "       -0.18245578, -0.        , -0.        ,  0.        ,  0.08056222,\n",
       "        0.0656362 ,  0.18067522,  0.00173946,  0.        ,  0.02065158,\n",
       "        0.02433662, -0.        ,  0.1767727 ,  0.        ,  0.01161762,\n",
       "       -0.        , -0.        , -0.18004146, -0.        ,  0.0147099 ,\n",
       "        0.        ,  0.        ,  0.06646849, -0.16067734,  0.        ,\n",
       "        0.        ,  0.10267164,  0.11150776,  0.15527655,  0.        ,\n",
       "        0.        ,  0.18290897,  0.        ,  0.11125077,  0.14937095,\n",
       "       -0.        , -0.00420108,  0.16608955,  0.14837117,  0.        ,\n",
       "        0.09318213, -0.15986633, -0.        ,  0.04242773,  0.        ,\n",
       "       -0.        , -0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        ,  0.        , -0.        ,  0.        ,\n",
       "       -0.09592042,  0.        , -0.        , -0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.02152976, -0.        , -0.        ,\n",
       "       -0.10259014, -0.16574502, -0.        ,  0.        , -0.        ,\n",
       "       -0.        ,  0.        , -0.        ,  0.        , -0.17510557,\n",
       "        0.01584105, -0.05838689, -0.        , -0.        , -0.17603575,\n",
       "        0.        ,  0.        , -0.        , -0.1465838 ,  0.        ,\n",
       "       -0.        , -0.18253402,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.        , -0.        ,  0.0575297 ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret4[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
